{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amu2uTlP-BZJ"
      },
      "outputs": [],
      "source": [
        "# importing modules and functions to get data from the database\n",
        "!pip install mysql.connector\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', None)\n",
        "import mysql.connector\n",
        "import numpy as np\n",
        "pd.set_option('display.max_rows', 500)\n",
        "\n",
        "class MysqlIO:\n",
        "    \"\"\"Connect to MySQL server with python and excecute SQL commands.\"\"\"\n",
        "    def __init__(self, database):\n",
        "        try:\n",
        "            connection = mysql.connector.connect(host='db.americor.consultancy.cmlinsight.com',\n",
        "                                                 database=database,\n",
        "                                                 user='admin',\n",
        "                                                 password='admin@123',\n",
        "                                                 use_pure=True\n",
        "                                                 )\n",
        "            if connection.is_connected():\n",
        "                db_info = connection.get_server_info()\n",
        "                print(\"Connected to MySQL Server version\", db_info)\n",
        "                print(\"Your're connected to database:\", database)\n",
        "                self.connection = connection\n",
        "        except Exception as e:\n",
        "            print(\"Error while connecting to MySQL\", e)\n",
        "            \n",
        "    def execute(self, query, header=False):\n",
        "        \"\"\"Execute SQL commands and return retrieved queries.\"\"\"\n",
        "        cursor = self.connection.cursor(buffered=True)\n",
        "        cursor.execute(query)\n",
        "        try:\n",
        "            record = cursor.fetchall()\n",
        "            if header:\n",
        "                header = [i[0] for i in cursor.description]\n",
        "                return {'header': header, 'record': record}\n",
        "            else:    \n",
        "                return record\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "    def to_df(self, query):\n",
        "        \"\"\"Return the retrieved SQL queries into pandas dataframe.\"\"\"\n",
        "        res = self.execute(query, header=True)\n",
        "        df = pd.DataFrame(res['record'])\n",
        "        df.columns = res['header']\n",
        "        return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJOvtNUy9Ua0"
      },
      "source": [
        "# Database Connection and merging necessary tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVNUAWcd-QiU"
      },
      "outputs": [],
      "source": [
        "analytics_database = MysqlIO('americor_analytics')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMctSq3a-Qkt"
      },
      "outputs": [],
      "source": [
        "enroll_deposite_df  = analytics_database.to_df('SELECT * from mvd_fe_customer_profile_enroll_deposit order by customer_id ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VZqNAx763hb"
      },
      "outputs": [],
      "source": [
        "enroll_deposite_df.info(verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fB1NbHY--Qmr"
      },
      "outputs": [],
      "source": [
        "Cancelled_df  = analytics_database.to_df('SELECT * from mvd_customer_lifecycle_full WHERE customer_type_value = \"cancel\";')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTvJHACm-RQt"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "t_date = datetime.datetime(2018, 5, 1)\n",
        "print(t_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25Ukihl78cXs"
      },
      "outputs": [],
      "source": [
        "#Droping other coloumns from lifecycle table before merging\n",
        "Cancelled_df = Cancelled_df[['customer_id','customer_type_event_ts','customer_type_value']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enroll_deposite_df['enroll_deposit_ts']=pd.to_datetime(enroll_deposite_df['enroll_deposit_ts'])"
      ],
      "metadata": {
        "id": "gSygGGOsVWrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#selecting entries which belongs to date After 2k18 May\n",
        "Cancelled_df= Cancelled_df[Cancelled_df['customer_type_event_ts'] > t_date]\n",
        "enroll_deposite_df= enroll_deposite_df[enroll_deposite_df['enroll_deposit_ts'] > t_date]"
      ],
      "metadata": {
        "id": "JZ9fl1PbVQXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xY8CEJWv-RXa"
      },
      "outputs": [],
      "source": [
        "merged_data=pd.merge(enroll_deposite_df, Cancelled_df, on=['customer_id'], how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIRa-yS--RcC"
      },
      "outputs": [],
      "source": [
        "merged_data.rename(columns = {'customer_type_value':'cancel'}, inplace = True)\n",
        "merged_data.rename(columns = {'customer_type_event_ts':'cancel_ts'}, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISIExK6u8JJ2"
      },
      "outputs": [],
      "source": [
        "merged_data['cancel'] = merged_data['cancel'].fillna(0)\n",
        "merged_data['cancel'] =  merged_data['cancel'].replace(['cancel'],1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Leomn618JMT"
      },
      "outputs": [],
      "source": [
        "merged_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5861-u-08JOf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn2KUNgH9SCR"
      },
      "source": [
        "# Creating nre features (Age)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofdS-h9SYWui"
      },
      "outputs": [],
      "source": [
        "merged_data['dob']= pd.to_datetime(merged_data['dob'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FCPr271ZjRJ"
      },
      "outputs": [],
      "source": [
        "merged_data['dob'].replace(r'^\\s*$', np.nan, regex=True,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSuFty_FZtlh"
      },
      "outputs": [],
      "source": [
        "merged_data['dob']=merged_data['dob'].fillna(merged_data['dob'].mode()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TftTEvjFpQhJ"
      },
      "outputs": [],
      "source": [
        "merged_data['Age'] = merged_data['enroll_deposit_ts'].dt.year-merged_data['dob'].dt.year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edREKPDY8JTf"
      },
      "outputs": [],
      "source": [
        "merged_data.drop('dob',axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xUOnYzP8JV7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmaX6dcr-DRI"
      },
      "source": [
        "# data Inspection and columns droping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nrd51fFk-RvK"
      },
      "outputs": [],
      "source": [
        "data=merged_data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5QuVXu3-R0K"
      },
      "outputs": [],
      "source": [
        "data['cancel'].value_counts() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEaIgRSu-R3C"
      },
      "outputs": [],
      "source": [
        "cancel_rate=32427/len(data)\n",
        "cancel_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jnRe9zo-R5q"
      },
      "outputs": [],
      "source": [
        "# Remove completely null columns\n",
        "def get_null_cols(df):\n",
        "    null_cols = set()\n",
        "    num_records = len(df)\n",
        "    for col in df.columns:\n",
        "        if df[col].isna().sum() == num_records:\n",
        "            null_cols.add(col)\n",
        "    return null_cols\n",
        "\n",
        "null_cols = get_null_cols(data)\n",
        "print(len(null_cols))\n",
        "print(null_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YorqMoZ9-R8i"
      },
      "outputs": [],
      "source": [
        "# ids and timestamps are to be removed\n",
        "manual_drop_cols = ['customer_id','lead_ts','primary_applicant_id','prev_event_ts','cancel_ts','enroll_ts','enroll_deposit_ts','email','edcp_credit_report_ts','edcp_prev_event_ts',\n",
        "            #features with confirmed leakage\n",
        "            'edex_extract_date', 'quality', 'detailed_hardship_reason','edex_ZIP_CD','budget_note','schedule_payments_type'\n",
        "]  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Y_3D0MS-R-9"
      },
      "outputs": [],
      "source": [
        "data.drop(null_cols,axis=1,inplace=True)\n",
        "data.drop(manual_drop_cols, axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYl3zfZf-SBn"
      },
      "outputs": [],
      "source": [
        "#replacing null for empty strings\n",
        "for i in data.columns:\n",
        "    data[i].replace(r'^\\s*$', np.nan, regex=True,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# handle outliers\n",
        "def handle_outliers(df, cols, thresholds):\n",
        "    \"\"\"\n",
        "    For given set of columns, there are thresholds defined to identify outliers. The thresholds are passed\n",
        "    as a parameter. If a column value of a particular row is larger than the threshold, the value is marked\n",
        "    as None.\n",
        "    :param cols: list of column names\n",
        "    :param thresholds: list of threshold for each of column specified in the list of column names.\n",
        "    \"\"\"\n",
        "    \n",
        "    for i in range(len(cols)):\n",
        "        for j in range(len(df)):\n",
        "            if df[cols[i]].iloc[j] >= thresholds[i]:\n",
        "                df[cols[i]].iloc[j] = None\n",
        "               \n",
        "                \n",
        "    return df\n",
        "\n",
        "cols_with_outliers = ['edex_P13_ALL7517', 'edex_P13_BCA7600', 'edex_P13_BCA8122', 'edex_P13_BCC3520', 'edex_P13_BCC5627',\n",
        "                      'edex_P13_BCC7140', 'edex_P13_BCC7141', 'edex_P13_PIL5020', 'edex_P13_REH0437', 'edex_P13_REH5530',\n",
        "                      'edex_UNSCD_RATIO_V1']\n",
        "outlier_thresholds = [990, 990, 9900, 90, 999999990, 990, 990, 999999990, 999999990, 999999990, 100]\n",
        "data = handle_outliers(data, cols_with_outliers, outlier_thresholds)"
      ],
      "metadata": {
        "id": "lzdtOf7JXEzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HK0141ul-SEO"
      },
      "outputs": [],
      "source": [
        "n_samples = data.shape[0]\n",
        "dropped_cols_90 = []\n",
        "for col in data.columns:\n",
        "    null_frac = data[col].isnull().sum() / n_samples\n",
        "    if null_frac >= 0.9:\n",
        "        dropped_cols_90.append(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qf2bIjW8_Nc7"
      },
      "outputs": [],
      "source": [
        "len(dropped_cols_90)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wsl8AdSk2mK"
      },
      "outputs": [],
      "source": [
        "dropped_cols_90"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlngFrvG-SIM"
      },
      "outputs": [],
      "source": [
        "data.drop(dropped_cols_90, axis=1, inplace=True) #droped columns with 90% Null value"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['edex_EMAIL_IND'].value_counts()"
      ],
      "metadata": {
        "id": "CiIT8usX33Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Y and N coloumns are marked as null so convert as 0 and 1 to make use of this feature\n",
        "data['edex_EMAIL_IND'] = data['edex_EMAIL_IND'].fillna(0)\n",
        "data['edex_EMAIL_IND'] = data['edex_EMAIL_IND'].replace(['Y'],1)"
      ],
      "metadata": {
        "id": "4w1nOq423rD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZscLPYalymO"
      },
      "outputs": [],
      "source": [
        "def drop_no_info_cols(df):\n",
        "    \"\"\"\n",
        "    Drop columns that include only one value for all the rows\n",
        "    :param df: dataframe\n",
        "    :return df: processed df\n",
        "    \"\"\"\n",
        "    unique_count = df.nunique()\n",
        "    unique_check = unique_count[unique_count == 1]\n",
        "    df_cols = df.columns\n",
        "    cols_to_drop = df_cols[unique_count == 1]\n",
        "    return cols_to_drop\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUTF0H8YmC-X"
      },
      "outputs": [],
      "source": [
        "no_info_cols = drop_no_info_cols(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VISuEznVmuCb"
      },
      "outputs": [],
      "source": [
        "data.drop(no_info_cols,axis=1,inplace=True) #removing coloumns that have only 1 valur throughout the coloumn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jmo4e11CnA2k"
      },
      "outputs": [],
      "source": [
        "data.info(verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRiA8blp-SKD"
      },
      "outputs": [],
      "source": [
        "def fill_all_nulls(df):\n",
        "    data = df.copy()\n",
        "    n_samples = data.shape[0]\n",
        "    for col in data.columns:\n",
        "        null_frac = data[col].isnull().sum() / n_samples\n",
        "        if null_frac > 0:\n",
        "          if data[col].dtypes == 'object':\n",
        "            data[col] = data[col].fillna('Unknown')\n",
        "          else:\n",
        "            mean_val = np.mean(data[col])\n",
        "            data[col] = data[col].fillna(mean_val)\n",
        "    return data\n",
        "\n",
        "data = fill_all_nulls(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLehuT-PA4wm"
      },
      "outputs": [],
      "source": [
        "Numerical_cols=[]\n",
        "obje_cols=[]\n",
        "for col in data.columns:\n",
        "  if data[col].dtypes == 'object':\n",
        "    obje_cols.append(col)\n",
        "  else:\n",
        "    Numerical_cols.append(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjkiVxDRohs-"
      },
      "outputs": [],
      "source": [
        "obje_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Woc7nOk5ovXW"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7JGiUfxowlM"
      },
      "source": [
        "# MOM score value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ7z2rug2lYk"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def columns_without_targets(data):\n",
        "    colmns_without_targets = list(data.columns)\n",
        "    #colmns_without_targets.remove('enroll_flag_timestamp')\n",
        "    colmns_without_targets.remove('cancel')\n",
        "    return colmns_without_targets"
      ],
      "metadata": {
        "id": "WdXkU8me22kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM5J6X5e2lYk"
      },
      "outputs": [],
      "source": [
        "def get_categorical_mom_score_with_na(mom_df, feature, outcome_variable):\n",
        "    \n",
        "    not_null_mom_df = mom_df[~mom_df[feature].isnull()]\n",
        "    null_mom_df = mom_df[mom_df[feature].isnull()]\n",
        "\n",
        "    not_null_1_df = not_null_mom_df[not_null_mom_df[outcome_variable] == 1].groupby(feature)[outcome_variable].count().to_frame()\n",
        "    not_null_1_df = not_null_1_df.reset_index()\n",
        "    not_null_1_df.columns = ['feature', '1_count']    \n",
        "    \n",
        "    null_1_count = null_mom_df[null_mom_df[outcome_variable] == 1].shape[0]\n",
        "    \n",
        "    not_null_0_df = not_null_mom_df[not_null_mom_df[outcome_variable] == 0].groupby(feature)[outcome_variable].count().to_frame()\n",
        "    not_null_0_df = not_null_0_df.reset_index()\n",
        "    not_null_0_df.columns = ['feature', '0_count']    \n",
        "    \n",
        "    null_0_count = null_mom_df[null_mom_df[outcome_variable] == 0].shape[0]\n",
        "    \n",
        "    count_df = not_null_1_df.merge(not_null_0_df, how='outer', on='feature')\n",
        "    count_df['feature'] = count_df['feature'].apply(str)    \n",
        "    \n",
        "    null_count_dict = {'feature':'Null', '1_count': null_1_count,  '0_count': null_0_count}\n",
        "    count_df = count_df.append(null_count_dict, ignore_index=True)   \n",
        "    \n",
        "    count_df = count_df.fillna(0)\n",
        "    \n",
        "    sum_1 = count_df['1_count'].sum()\n",
        "    sum_0 = count_df['0_count'].sum()    \n",
        "\n",
        "    count_df['1_frac'] = count_df['1_count']/sum_1\n",
        "    count_df['0_frac'] = count_df['0_count']/sum_0  \n",
        "    \n",
        "    count_df['overlap'] = count_df.apply(lambda row: min(row['1_frac'], row['0_frac']), axis=1)\n",
        "    \n",
        "    mom_score_with_na = round(count_df['overlap'].sum(), 2)\n",
        "    \n",
        "    return mom_score_with_na"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqWOrPwR2lYl"
      },
      "outputs": [],
      "source": [
        "def get_numerical_mom_score_with_na(fm,feature,target,Nbins):\n",
        "    #--let's add NA\n",
        "    warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
        "    persisted_id = fm[target] == 1\n",
        "    not_persisted_id = fm[target] == 0\n",
        "    Npersist = np.sum(persisted_id)\n",
        "    Nnon_persist = np.sum(not_persisted_id)\n",
        "    null_frac = fm[feature].isnull().sum()/len(fm[target])\n",
        "    #print(feature, null_frac)\n",
        "    #print(bin_min, bin_max)\n",
        "    if fm[feature].dtypes == 'object':\n",
        "        p00 = []\n",
        "        p11 = []\n",
        "        binn = []\n",
        "        mom = []\n",
        "    else:\n",
        "        p1 = fm[feature][persisted_id]\n",
        "        p0 = fm[feature][not_persisted_id]\n",
        "        bin_max = np.max(fm[feature])\n",
        "        bin_min = np.min(fm[feature])\n",
        "        if null_frac > 0:\n",
        "            delta_val = (bin_max - bin_min)/Nbins\n",
        "            replace_val = bin_max + delta_val*5\n",
        "            bin_max = replace_val\n",
        "            tmp = fm[feature].fillna(replace_val)\n",
        "            p1 = tmp[persisted_id]\n",
        "            p0 = tmp[not_persisted_id]\n",
        "        binn = np.linspace(bin_min,bin_max,Nbins)\n",
        "        p00, bin0 = np.histogram(p0, binn)/(Nnon_persist*1.0)\n",
        "        p11, bin1 = np.histogram(p1, binn)/(Npersist*1.0)\n",
        "        mom = np.sum(np.minimum(p00,p11))\n",
        "        mom = round(mom, 2)\n",
        "\n",
        "    return mom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emzCDMjJ2lYl"
      },
      "outputs": [],
      "source": [
        "mom_data= data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check mom score\n",
        "drop_features_dict = {}\n",
        "drop_features = []\n",
        "feature_mom={}\n",
        "for columnName in columns_without_targets(data):\n",
        "    if data[columnName].dtypes == 'object':\n",
        "        mom = get_categorical_mom_score_with_na(data, columnName, 'cancel')\n",
        "   \n",
        "    else:\n",
        "        mom = get_numerical_mom_score_with_na(data, columnName,'cancel',30)\n",
        "    \n",
        "    if abs(mom)<=35e-2:\n",
        "        drop_features.append(columnName)\n",
        "        drop_features_dict[columnName]= mom \n",
        "    else:\n",
        "        feature_mom[columnName]= mom "
      ],
      "metadata": {
        "id": "_Nqnb-Rj_lVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(drop_features_dict.items(), columns=['Dropped Feature', 'mom_score']).sort_values(by=['mom_score'])"
      ],
      "metadata": {
        "id": "UCnNoHAO_n6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mom_score_df = pd.DataFrame(feature_mom.items(), columns=['Feature', 'mom_score']).sort_values(by=['mom_score'])"
      ],
      "metadata": {
        "id": "i3YhElpiHsgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnwDXGX32lYl"
      },
      "outputs": [],
      "source": [
        "mom_score_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3LexhDI2lYl"
      },
      "outputs": [],
      "source": [
        "#removing features baesd on the mom score. features having mom score less than 0.35 are removed\n",
        "data.drop(drop_features,axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGHVDYvhp_qu"
      },
      "source": [
        "# Data preprocessing ctd.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fe1S12yKxsBR"
      },
      "outputs": [],
      "source": [
        "data.drop(['edex_CITY_NAME','zip'],axis=1,inplace=True) #cannot effectivly represent as numerical coloumns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kM_8ikJHyCKW"
      },
      "outputs": [],
      "source": [
        "Numerical_cols=[]\n",
        "obje_cols=[]\n",
        "for col in data.columns:\n",
        "  if data[col].dtypes == 'object':\n",
        "    obje_cols.append(col)\n",
        "  else:\n",
        "    Numerical_cols.append(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_5QBzldvGRH"
      },
      "outputs": [],
      "source": [
        "def encode_cateogrical_cols_v2(df):\n",
        "    categorical_cols = df.select_dtypes(include='object')\n",
        "    for col in categorical_cols:\n",
        "        df[col], uniques = pd.factorize(df[col])\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0iwMm3Hxb-x"
      },
      "outputs": [],
      "source": [
        "data = encode_cateogrical_cols_v2(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model building"
      ],
      "metadata": {
        "id": "PZqbYU7XZX_t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3trrxLawB_vo"
      },
      "outputs": [],
      "source": [
        "df =data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G-IgjrpB_1l"
      },
      "outputs": [],
      "source": [
        "X = df.loc[:, df.columns != 'cancel']\n",
        "y = df.loc[:, df.columns == 'cancel']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIWuy7MxD-yT"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2350, stratify=y,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_tYYW2DEAUG"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhxA7LEyEAqn"
      },
      "outputs": [],
      "source": [
        "y_train['cancel'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uu77smuMEAtR"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'n_estimators': 100,\n",
        "    'max_features': 0.33,\n",
        "    'max_depth': 8,\n",
        "    'random_state' : 0,\n",
        "   }\n",
        "rf_model = RandomForestClassifier(**params)\n",
        "rf_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcNw8zFaEAx6"
      },
      "outputs": [],
      "source": [
        "y_pred_prob = rf_model.predict_proba(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLymLr-ZEA1E"
      },
      "outputs": [],
      "source": [
        "y_pred = rf_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AKKNflEEMSu"
      },
      "outputs": [],
      "source": [
        "y_pred_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoEAm9scEJGx"
      },
      "outputs": [],
      "source": [
        "y_pred_prob = y_pred_prob[:, 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJqrcF6fEr_Q"
      },
      "outputs": [],
      "source": [
        "fpr, tpr, thresholds = metrics.roc_curve(y_test,  y_pred_prob)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGUaQOFJEsDh"
      },
      "outputs": [],
      "source": [
        "auc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "auc_str = \"{:0.3f}\".format(auc)\n",
        "sns.set(context=\"paper\", font_scale=1.2)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, [0, 1], [0, 1])\n",
        "plt.xlabel('False positives')\n",
        "plt.ylabel('True positives')\n",
        "plt.title('ROC curve with AUC = ' + auc_str)"
      ],
      "metadata": {
        "id": "iek2hu3abNwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_curve, auc, precision_recall_curve, precision_score, \\\n",
        "    recall_score, confusion_matrix\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.calibration import calibration_curve"
      ],
      "metadata": {
        "id": "B4x_vhxGgNq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Several key measurements are taken to measrure the model performance. Accuracy, precision, recall and f1 scores.\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "print(\"Accuracy\", accuracy)\n",
        "print(\"Precision\", precision)\n",
        "print(\"Recall\", recall)\n",
        "print(\"F1\", f1)"
      ],
      "metadata": {
        "id": "-nvcADrXe4Io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## This is a helper function\n",
        "def draw_plot(x_label, y_label, title):\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "q58DlKCqbNwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
        "prc_auc = auc(recall, precision)\n",
        "auc_str = \"{:0.3f}\".format(prc_auc)\n",
        "baseline = y_test.sum() / len(y_test) * 1.0\n",
        "plt.plot(recall, precision, [0, 1], [baseline, baseline])\n",
        "draw_plot('Recall', 'Precision', 'PRC with AUC = ' + auc_str)"
      ],
      "metadata": {
        "id": "xmrvLj3nbNwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiKrbpf7EsKP"
      },
      "outputs": [],
      "source": [
        "rf_feature_importance = rf_model.feature_importances_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YtmYq7IFef4"
      },
      "outputs": [],
      "source": [
        "rf_model.n_features_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kv4O_NqRFeie"
      },
      "outputs": [],
      "source": [
        "feature_names = list(df.columns)\n",
        "feature_names.remove('cancel')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance_df = pd.DataFrame.from_dict(dict(zip(feature_names, rf_feature_importance)), orient='index')"
      ],
      "metadata": {
        "id": "hPQKy3Y2Fek0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance_df = feature_importance_df.reset_index()\n",
        "feature_importance_df.columns = ['feature', 'importance']\n",
        "feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)"
      ],
      "metadata": {
        "id": "xkFwxMr3Fenc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importances = {'Feature':X_train.columns,'Feature importance':rf_model.feature_importances_}\n",
        "df_feature_importances = pd.DataFrame(feature_importances)\n",
        "df_feature_importances = df_feature_importances.sort_values(by=['Feature importance'], ascending=False)\n",
        "df_feature_importances = df_feature_importances.merge(mom_score_df , on=\"Feature\", how=\"left\", indicator=False)\n",
        "df_feature_importances"
      ],
      "metadata": {
        "id": "AiDD8Ps2WnJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features_by_importance(classifier):\n",
        "    \"\"\"\n",
        "    Get top 20 most of important features for the RF model to predict the label.\n",
        "    \"\"\"\n",
        "    feature_importance_values = classifier.feature_importances_\n",
        "    indices = np.argsort(feature_importance_values)[-20:]\n",
        "    return feature_importance_values, indices\n",
        "\n",
        "def draw_feature_importance(classifier, features):\n",
        "    \"\"\"\n",
        "    Draw the level of importance of each feature for RF model to predict the label. Top 20 most importance\n",
        "    features is considered\n",
        "\n",
        "    :param features: list of feature names\n",
        "    \"\"\"\n",
        "    feature_importance_values, indices = get_features_by_importance(classifier)\n",
        "    plt.rcParams['font.size'] = '10'  # This does not work\n",
        "    plt.title('Feature Importance')\n",
        "    plt.barh(range(len(indices)), feature_importance_values[indices], color='b', align='center')\n",
        "    plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "FoVBan_IFetL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f=df.columns.drop('cancel')\n",
        "draw_feature_importance(rf_model,f)"
      ],
      "metadata": {
        "id": "gGM9lUd5F4b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Good-feature class-conditional distributions with MOM"
      ],
      "metadata": {
        "id": "7FEmNwAVOV11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def class_cond_pdf2(fm,feature,target,Nbins):\n",
        "\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n",
        "\n",
        "    persisted_id = fm[target] == 1\n",
        "    not_persisted_id = fm[target] == 0\n",
        "    Npersist = np.sum(persisted_id)\n",
        "    Nnon_persist = np.sum(not_persisted_id)\n",
        "    if type(feature) == str:\n",
        "        p1 = fm[feature][persisted_id] \n",
        "        p0 = fm[feature][not_persisted_id] \n",
        "        bin_max = np.max(fm[feature])\n",
        "        bin_min = np.min(fm[feature])\n",
        "    else:\n",
        "        p1 = feature[persisted_id]\n",
        "        p0 = feature[not_persisted_id]\n",
        "        bin_max = np.max(feature)\n",
        "        bin_min = np.min(feature)\n",
        "    print(p0)\n",
        "    print(Npersist)\n",
        "\n",
        "    #print(bin_min, bin_max)\n",
        "                 \n",
        "    binn = np.linspace(bin_min,bin_max,Nbins)\n",
        "    p00, bin0 = np.histogram(p0, binn)/(Nnon_persist*1.0)\n",
        "    p11, bin1 = np.histogram(p1, binn)/(Npersist*1.0)\n",
        "    mom = np.sum(np.minimum(p00,p11))\n",
        "    plt.plot(binn[0:Nbins-1],p00, binn[0:Nbins-1],p11)\n",
        "    momStr = \"{:0.3f}\".format(mom)\n",
        "    if type(feature) == str:\n",
        "        plt.title('PDFs for ' + feature + ' MOM = ' + momStr, fontsize = 16)\n",
        "    else:\n",
        "        plt.title('PDFs for prediction: ' + ' MOM = ' + momStr, fontsize = 16)\n",
        "    plt.show()\n",
        "    \n",
        "    return p00, p11, binn[0:Nbins-1], mom"
      ],
      "metadata": {
        "id": "R7TdtbEYOhxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_features = list(df_feature_importances['Feature'].values)[:20]"
      ],
      "metadata": {
        "id": "M262T3c7OD8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for feature in top_features[:10]:\n",
        "    p00, binn, enroll_means, mom = class_cond_pdf2(data , feature ,\"cancel\", 30)"
      ],
      "metadata": {
        "id": "V_4tKmYjOMbY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Enroll_deposite to cancel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}